{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from utils.evaluation_framework import EvaluationFramework\n",
    "\n",
    "random.seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "df_real_data = pd.read_csv(\"./Data/Real/clean_data.csv\")\n",
    "\n",
    "d_synthetic_data = {}\n",
    "d_synthetic_data[\"CTGAN\"] = pd.read_csv(f\"./Data/Synthetic/ctgan_data.csv\")\n",
    "d_synthetic_data[\"TVAE\"] = pd.read_csv(f\"./Data/Synthetic/tvae_data.csv\")\n",
    "d_synthetic_data[\"COP\"] = pd.read_csv(f\"./Data/Synthetic/cop_synthetic_data.csv\")\n",
    "d_synthetic_data[\"Gauss\"] = pd.read_csv(f\"./Data/Synthetic/gauss_data.csv\")\n",
    "d_synthetic_data[\"GReaT\"] = pd.read_csv(f\"./Data/Synthetic/great_data.csv\")\n",
    "\n",
    "# Get a list with categorical features' names\n",
    "categorical_features = [\n",
    "    feature for feature in df_real_data.select_dtypes(include=\"object\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the mappings for each feature\n",
    "mappings = {}\n",
    "\n",
    "# Iterate over each categorical feature to create the mappings\n",
    "for feature in categorical_features:\n",
    "    # Fit the LabelEncoder to the real data\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(df_real_data[feature])\n",
    "    \n",
    "    # Create the mapping dictionary\n",
    "    mappings[feature] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    # Apply the mapping to the real data\n",
    "    df_real_data[feature] = df_real_data[feature].map(mappings[feature])\n",
    "    \n",
    "    # Apply the mapping to each synthetic dataset\n",
    "    for dataset in d_synthetic_data:\n",
    "        d_synthetic_data[dataset][feature] = d_synthetic_data[dataset][feature].map(mappings[feature])\n",
    "\n",
    "# In case there are any NaN values after mapping (for unseen labels in synthetic data), handle them accordingly\n",
    "for feature in categorical_features:\n",
    "    df_real_data[feature].fillna(-1, inplace=True)\n",
    "    for dataset in d_synthetic_data:\n",
    "        d_synthetic_data[dataset][feature].fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = EvaluationFramework(df_real_data, d_synthetic_data, categorical_features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n",
      "c:\\Users\\nbasa\\GitHub\\mini_ai\\synthetic_data_evaluation_framework\\utils\\evaluation_framework.py:84: RuntimeWarning: invalid value encountered in divide\n",
      "  cramers_v = np.sqrt(chi2_stat / (n * min(k - 1, r - 1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein/Cramers-v test\n",
      "--------------------------------------------------\n",
      "GReaT score: 27.5500\n",
      "TVAE score: 46.7000\n",
      "Gauss score: 58.7500\n",
      "CTGAN score: 59.1500\n",
      "COP score: 60.3500\n",
      "\n",
      "\n",
      "Novelty test\n",
      "--------------------------------------------------\n",
      "CTGAN score: 0.0000\n",
      "TVAE score: 0.0000\n",
      "COP score: 0.0000\n",
      "Gauss score: 0.0000\n",
      "GReaT score: 0.0864\n",
      "\n",
      "\n",
      "Anomaly detection test\n",
      "--------------------------------------------------\n",
      "CTGAN score: 0.0000\n",
      "TVAE score: 0.0000\n",
      "COP score: 0.0000\n",
      "Gauss score: 0.0000\n",
      "GReaT score: 0.0001\n",
      "\n",
      "\n",
      "Method: CTGAN\n",
      "> (Train) AUC score: 100.00\n",
      "> (Test) AUC score: 100.00\n",
      "Method: TVAE\n",
      "> (Train) AUC score: 100.00\n",
      "> (Test) AUC score: 100.00\n",
      "Method: COP\n",
      "> (Train) AUC score: 100.00\n",
      "> (Test) AUC score: 100.00\n",
      "Method: Gauss\n",
      "> (Train) AUC score: 100.00\n",
      "> (Test) AUC score: 100.00\n",
      "Method: GReaT\n",
      "> (Train) AUC score: 100.00\n",
      "> (Test) AUC score: 99.99\n",
      "\n",
      "\n",
      "Domain classifier test\n",
      "--------------------------------------------------\n",
      "CTGAN score: 100.00\n",
      "TVAE score: 100.00\n",
      "COP score: 100.00\n",
      "Gauss score: 100.00\n",
      "GReaT score: 99.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wasserstein/Cramers-v test\n",
    "score_wasserstein_cramers_v = evaluation.wasserstein_cramers_v_test()\n",
    "\n",
    "# Novelty test\n",
    "score_novelty = evaluation.novelty_test()\n",
    "\n",
    "# Anomaly detection test\n",
    "score_anomaly = evaluation.anomaly_detection()\n",
    "\n",
    "# Domain classifier test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "xgbc = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "score_classification = evaluation.domain_classifier(model=xgbc, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] H0: {All methods exhibited similar results with no statistical differences}\n",
      "[INFO] FAR: 2.171 (p-value: 0.70426) - H0 is failed to be rejected)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>FAR</th>\n",
       "      <th>APV</th>\n",
       "      <th>Null hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TVAE</td>\n",
       "      <td>7.25</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GReaT</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gauss</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTGAN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COP</td>\n",
       "      <td>12.25</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Methods    FAR       APV Null hypothesis\n",
       "0    TVAE   7.25         -               -\n",
       "1   GReaT   9.25  0.000002        Rejected\n",
       "2   Gauss  11.75  0.000004        Rejected\n",
       "3   CTGAN  12.00  0.000009        Rejected\n",
       "4     COP  12.25    0.0455        Rejected"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistical analysis results\n",
    "Ranking = evaluation.get_synthesizers_ranking()\n",
    "display(Ranking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
